{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class object for Deep Q-Learning\n",
    "class DQN:\n",
    "    def __init__(self, env, gamma = 0.99, lr = 5e-4, tau = 1e-3):\n",
    "        # init environment\n",
    "        self.env = env\n",
    "        \n",
    "        # Init replay buffer\n",
    "        # Use deque for its property of auto-discarding stored elements if it is full\n",
    "        self.replay_buffer  = deque(maxlen=int(1e5))\n",
    "        \n",
    "        # define discount rate gamma\n",
    "        # learning rate lr for keras gradient descent\n",
    "        # tau for soft-update\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau \n",
    "        \n",
    "        # construct model for action-value function\n",
    "        # construct target_model for target action-value function\n",
    "        # target model is used to generate value estimate and make action selection\n",
    "        # model is learning from experience and improve estimate\n",
    "        self.model, self.target_model  = self.init_model(), self.init_model()\n",
    "        \n",
    "        # Every C step soft update target model Q_hat to model Q\n",
    "        self.C = 4\n",
    "        \n",
    "        # batch size in learning\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # Counter\n",
    "        self.t = 0\n",
    "    \n",
    "    # create keras NN model\n",
    "    def init_model(self):\n",
    "        # set input and output shape\n",
    "        # input shape is the dimension of state space (8,1)\n",
    "        # output shape is the dimension of actions (4,1)\n",
    "        input_shape = self.env.observation_space.shape[0]\n",
    "        output_shape = self.env.action_space.n\n",
    "        \n",
    "        # Define Sequential model (linear stack of layers)  \n",
    "        # Add hidden layers: two hidden layers with the size of 64\n",
    "        model = Sequential()        \n",
    "        model.add(Dense(64, input_dim=input_shape, activation=\"relu\"))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(output_shape))\n",
    "        \n",
    "        # Define the loss function of mse, Use Adam for optimization\n",
    "        model.compile(loss=\"mean_squared_error\",optimizer=Adam(lr=self.lr))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Store transition to replay buffer\n",
    "    def add_to_buffer(self, state, action, reward, new_state, done):\n",
    "        self.replay_buffer.append([state, action, reward, new_state, done])\n",
    "    \n",
    "    # Take action with epsilon-greedy respect to action-value function prediction from model\n",
    "    def generate_action(self, state, eps):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    # the batch learning (gradient descent) of the model Q\n",
    "    def learning(self):\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.replay_buffer) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.replay_buffer, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "    \n",
    "    # save complete model\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(gamma = 0.99, lr = 5e-4, tau = 1e-3, epsilon_decay = 0.995):\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    # Define the epsilon and its decay for epsilon-greedy action selection\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = epsilon_decay\n",
    "    \n",
    "    # Define no.of training episodes and max steps for each episode\n",
    "    episodes  = 1000\n",
    "    steps = 500\n",
    "\n",
    "    # scores store all score in each trial\n",
    "    # scores_window stores last 100 trial scores\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    # epsilons store all epsilon values:\n",
    "    epsilons = []\n",
    "    \n",
    "    # init the DQN agent\n",
    "    agent = DQN(env=env, gamma=gamma, lr=lr, tau=tau)\n",
    "    \n",
    "    # each trial\n",
    "    for trial in range(episodes):\n",
    "        score = 0\n",
    "        cur_state = env.reset().reshape(1,8)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # each step in an training trial/episode\n",
    "        for step in range(steps):\n",
    "            #env.render()\n",
    "            \n",
    "            # get action from agent and take that action\n",
    "            action = agent.generate_action(cur_state ,epsilon)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            new_state = new_state.reshape(1,8)\n",
    "            \n",
    "            # add experience to replay buffer\n",
    "            agent.add_to_buffer(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            # learning from current step\n",
    "            agent.learning()\n",
    "            cur_state = new_state\n",
    "        \n",
    "            if done:\n",
    "                break   \n",
    "        \n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # decay of epsilon, in early training, perform more exploration\n",
    "        # at later stage of training, perform more exploitation\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon_min, epsilon)\n",
    "        \n",
    "        print('\\rtrial {}\\t Mean Score: {:.3f} \\t with epsilon: {:.3f}'.format(trial, np.mean(scores_window), epsilon), end=\"\")\n",
    "        \n",
    "        if trial % 100 == 0:\n",
    "            print('\\rtrial {}\\t Mean Score: {:.3f}'.format(trial, np.mean(scores_window)))\n",
    "        \n",
    "        # stop if last 100 consecutive scores, save the model/agent\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            #agent.save_model(\"dqn_model.h5\")\n",
    "            print('\\n Achieve Mean Score of 200 for past 100 trials with total {:d} trial!\\tAverage Score: {:.3f}'.format(trial-100, np.mean(scores_window)))\n",
    "            break                     \n",
    "    \n",
    "    env.close()\n",
    "    return (scores,epsilons)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_values(values, xlabel, ylabel):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(values)), values)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_gamma = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma value: 0.500\n",
      "trial 0\t Mean Score: -176.320 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -150.772 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -109.021 \t with epsilon: 0.365\n",
      "trial 300\t Mean Score: -152.744 \t with epsilon: 0.221\n",
      "trial 400\t Mean Score: -168.642 \t with epsilon: 0.134\n",
      "trial 500\t Mean Score: -140.166 \t with epsilon: 0.081\n",
      "trial 600\t Mean Score: -95.457 \t with epsilon: 0.0492\n",
      "trial 700\t Mean Score: -119.281 \t with epsilon: 0.030\n",
      "trial 800\t Mean Score: -104.947 \t with epsilon: 0.018\n",
      "trial 900\t Mean Score: -67.085 \t with epsilon: 0.0117\n",
      "trial 999\t Mean Score: -60.585 \t with epsilon: 0.010gamma value: 0.623\n",
      "trial 0\t Mean Score: -47.748 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -167.249 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -124.365 \t with epsilon: 0.365\n",
      "trial 300\t Mean Score: -152.382 \t with epsilon: 0.221\n",
      "trial 400\t Mean Score: -121.317 \t with epsilon: 0.134\n",
      "trial 500\t Mean Score: -127.421 \t with epsilon: 0.081\n",
      "trial 600\t Mean Score: -112.137 \t with epsilon: 0.049\n",
      "trial 700\t Mean Score: -92.707 \t with epsilon: 0.0301\n",
      "trial 800\t Mean Score: -63.727 \t with epsilon: 0.018\n",
      "trial 900\t Mean Score: -32.336 \t with epsilon: 0.011\n",
      "trial 999\t Mean Score: -6.208 \t with epsilon: 0.010gamma value: 0.745\n",
      "trial 0\t Mean Score: -126.300 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -162.290 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -110.875 \t with epsilon: 0.365\n",
      "trial 300\t Mean Score: -79.750 \t with epsilon: 0.2217\n",
      "trial 400\t Mean Score: -66.944 \t with epsilon: 0.134\n",
      "trial 500\t Mean Score: -32.770 \t with epsilon: 0.081\n",
      "trial 600\t Mean Score: -14.227 \t with epsilon: 0.049\n",
      "trial 700\t Mean Score: -6.258 \t with epsilon: 0.0305\n",
      "trial 800\t Mean Score: 3.725 \t with epsilon: 0.0189\n",
      "trial 900\t Mean Score: -17.472 \t with epsilon: 0.011\n",
      "trial 999\t Mean Score: -2.707 \t with epsilon: 0.010gamma value: 0.867\n",
      "trial 0\t Mean Score: -149.707 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -154.656 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -106.559 \t with epsilon: 0.365\n",
      "trial 300\t Mean Score: -58.763 \t with epsilon: 0.2213\n",
      "trial 400\t Mean Score: -46.486 \t with epsilon: 0.134\n",
      "trial 500\t Mean Score: 1.674 \t with epsilon: 0.08139\n",
      "trial 600\t Mean Score: -14.330 \t with epsilon: 0.049\n",
      "trial 700\t Mean Score: 7.409 \t with epsilon: 0.03011\n",
      "trial 800\t Mean Score: -19.694 \t with epsilon: 0.018\n",
      "trial 900\t Mean Score: -22.641 \t with epsilon: 0.011\n",
      "trial 999\t Mean Score: -27.359 \t with epsilon: 0.010gamma value: 0.990\n",
      "trial 0\t Mean Score: -198.096 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -156.746 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -87.738 \t with epsilon: 0.3658\n",
      "trial 300\t Mean Score: -31.061 \t with epsilon: 0.221\n",
      "trial 400\t Mean Score: 41.443 \t with epsilon: 0.1341\n",
      "trial 438\t Mean Score: 61.079 \t with epsilon: 0.111"
     ]
    }
   ],
   "source": [
    "for gamma in np.linspace(0.5, 0.99, num=5):\n",
    "    print(\"gamma value: {:.3f}\\n\".format(gamma), end=\"\")\n",
    "    scores_gamma[gamma], _ = train(gamma = gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
