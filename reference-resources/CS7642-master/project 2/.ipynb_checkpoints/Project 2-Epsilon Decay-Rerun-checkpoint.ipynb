{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.utils import plot_model\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class object for Deep Q-Learning\n",
    "class DQN:\n",
    "    def __init__(self, env, gamma = 0.99, lr = 5e-4, tau = 1e-3):\n",
    "        # init environment\n",
    "        self.env = env\n",
    "        \n",
    "        # Init replay buffer\n",
    "        # Use deque for its property of auto-discarding stored elements if it is full\n",
    "        self.replay_buffer  = deque(maxlen=int(1e5))\n",
    "        \n",
    "        # define discount rate gamma\n",
    "        # learning rate lr for keras gradient descent\n",
    "        # tau for soft-update\n",
    "        self.gamma = gamma\n",
    "        self.lr = lr\n",
    "        self.tau = tau \n",
    "        \n",
    "        # construct model for action-value function\n",
    "        # construct target_model for target action-value function\n",
    "        # target model is used to generate value estimate and make action selection\n",
    "        # model is learning from experience and improve estimate\n",
    "        self.model, self.target_model  = self.init_model(), self.init_model()\n",
    "        \n",
    "        # Every C step soft update target model Q_hat to model Q\n",
    "        self.C = 4\n",
    "        \n",
    "        # batch size in learning\n",
    "        self.batch_size = 64\n",
    "        \n",
    "        # Counter\n",
    "        self.t = 0\n",
    "    \n",
    "    # create keras NN model\n",
    "    def init_model(self):\n",
    "        # set input and output shape\n",
    "        # input shape is the dimension of state space (8,1)\n",
    "        # output shape is the dimension of actions (4,1)\n",
    "        input_shape = self.env.observation_space.shape[0]\n",
    "        output_shape = self.env.action_space.n\n",
    "        \n",
    "        # Define Sequential model (linear stack of layers)  \n",
    "        # Add hidden layers: two hidden layers with the size of 64\n",
    "        model = Sequential()        \n",
    "        model.add(Dense(64, input_dim=input_shape, activation=\"relu\"))\n",
    "        model.add(Dense(64, activation=\"relu\"))\n",
    "        model.add(Dense(output_shape))\n",
    "        \n",
    "        # Define the loss function of mse, Use Adam for optimization\n",
    "        model.compile(loss=\"mean_squared_error\",optimizer=Adam(lr=self.lr))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    # Store transition to replay buffer\n",
    "    def add_to_buffer(self, state, action, reward, new_state, done):\n",
    "        self.replay_buffer.append([state, action, reward, new_state, done])\n",
    "    \n",
    "    # Take action with epsilon-greedy respect to action-value function prediction from model\n",
    "    def generate_action(self, state, eps):\n",
    "        # epsilon-greey to take best action from action-value function\n",
    "        if np.random.random() < eps:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.model.predict(state)[0])\n",
    "    \n",
    "    # soft update to target model Q_hat from model Q\n",
    "    def train_target(self):\n",
    "        # target model and model are not updating at the same time\n",
    "        weights = self.model.get_weights()\n",
    "        target_weights = self.target_model.get_weights()\n",
    "        for i in range(len(target_weights)):\n",
    "            target_weights[i] = self.tau * weights[i] + (1 - self.tau) * target_weights[i]\n",
    "        # assign new weights to target model\n",
    "        self.target_model.set_weights(target_weights)\n",
    "\n",
    "    # the batch learning (gradient descent) of the model Q\n",
    "    def learning(self):\n",
    "        self.t = (self.t + 1) % self.C\n",
    "        \n",
    "        # update every C times and make sure buffer is filled with at least size batch size\n",
    "        if self.t == 0:\n",
    "            if len(self.replay_buffer) < self.batch_size: \n",
    "                return\n",
    "            \n",
    "            # init list states to store states \n",
    "            # init list of targets values forecast gernated by model Q associated with each state-action\n",
    "            states, targets_forecast = [], []\n",
    "            \n",
    "            # random sample from replay buffer\n",
    "            samples = random.sample(self.replay_buffer, self.batch_size)\n",
    "            \n",
    "            for state, action, reward, new_state, done in samples:\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    Q_new_state =  np.amax(self.target_model.predict(new_state)[0])\n",
    "                    target = reward + self.gamma *  Q_new_state\n",
    "\n",
    "                target_forecast = self.model.predict(state)\n",
    "                target_forecast[0][action] = target\n",
    "                \n",
    "                # append to lists for batch processing outside the iteartion\n",
    "                states.append(state[0])\n",
    "                targets_forecast.append(target_forecast[0])\n",
    "            \n",
    "            # batch learning to train the model Q   \n",
    "            self.model.fit(np.array(states), np.array(targets_forecast), epochs=1, verbose=0)\n",
    "            self.train_target()\n",
    "    \n",
    "    # save complete model\n",
    "    def save_model(self, fn):\n",
    "        self.model.save(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function for training\n",
    "def train(gamma = 0.99, lr = 5e-4, tau = 1e-3, epsilon_decay = 0.995):\n",
    "    env = gym.make(\"LunarLander-v2\")\n",
    "    \n",
    "    # Define the epsilon and its decay for epsilon-greedy action selection\n",
    "    epsilon = 1.0\n",
    "    epsilon_min = 0.01\n",
    "    epsilon_decay = epsilon_decay\n",
    "    \n",
    "    # Define no.of training episodes and max steps for each episode\n",
    "    episodes  = 1000\n",
    "    steps = 500\n",
    "\n",
    "    # scores store all score in each trial\n",
    "    # scores_window stores last 100 trial scores\n",
    "    scores = []                        \n",
    "    scores_window = deque(maxlen=100)\n",
    "    \n",
    "    # epsilons store all epsilon values:\n",
    "    epsilons = []\n",
    "    \n",
    "    # init the DQN agent\n",
    "    agent = DQN(env=env, gamma=gamma, lr=lr, tau=tau)\n",
    "    \n",
    "    # each trial\n",
    "    for trial in range(episodes):\n",
    "        score = 0\n",
    "        cur_state = env.reset().reshape(1,8)\n",
    "        epsilons.append(epsilon)\n",
    "        \n",
    "        # each step in an training trial/episode\n",
    "        for step in range(steps):\n",
    "            #env.render()\n",
    "            \n",
    "            # get action from agent and take that action\n",
    "            action = agent.generate_action(cur_state ,epsilon)\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            score += reward\n",
    "            new_state = new_state.reshape(1,8)\n",
    "            \n",
    "            # add experience to replay buffer\n",
    "            agent.add_to_buffer(cur_state, action, reward, new_state, done)\n",
    "            \n",
    "            # learning from current step\n",
    "            agent.learning()\n",
    "            cur_state = new_state\n",
    "        \n",
    "            if done:\n",
    "                break   \n",
    "        \n",
    "        scores.append(score)\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # decay of epsilon, in early training, perform more exploration\n",
    "        # at later stage of training, perform more exploitation\n",
    "        epsilon *= epsilon_decay\n",
    "        epsilon = max(epsilon_min, epsilon)\n",
    "        \n",
    "        print('\\rtrial {}\\t Mean Score: {:.3f} \\t with epsilon: {:.3f}'.format(trial, np.mean(scores_window), epsilon), end=\"\")\n",
    "        \n",
    "        if trial % 100 == 0:\n",
    "            print('\\rtrial {}\\t Mean Score: {:.3f}'.format(trial, np.mean(scores_window)))\n",
    "        \n",
    "        # stop if last 100 consecutive scores, save the model/agent\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            #agent.save_model(\"dqn_model.h5\")\n",
    "            print('\\n Achieve Mean Score of 200 for past 100 trials with total {:d} trial!\\tAverage Score: {:.3f}'.format(trial-100, np.mean(scores_window)))\n",
    "            break                     \n",
    "    \n",
    "    env.close()\n",
    "    return (scores,epsilons)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_values(values, xlabel, ylabel):\n",
    "    fig = plt.figure()\n",
    "    plt.plot(np.arange(len(values)), values)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_epsilon_decay = {}\n",
    "epsilons_epsilon_decay = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon_decays = [0.8, 0.9, 0.99, 0.995, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epsilon Decay Ratio: 0.800\n",
      "trial 0\t Mean Score: -70.613 \t with epsilon: 0.800\n",
      "trial 100\t Mean Score: -221.160 \t with epsilon: 0.010\n",
      "trial 200\t Mean Score: -43.831 \t with epsilon: 0.0100\n",
      "trial 300\t Mean Score: -26.366 \t with epsilon: 0.010\n",
      "trial 400\t Mean Score: -33.772 \t with epsilon: 0.010\n",
      "trial 500\t Mean Score: -36.628 \t with epsilon: 0.010\n",
      "trial 600\t Mean Score: -31.212 \t with epsilon: 0.010\n",
      "trial 700\t Mean Score: 6.946 \t with epsilon: 0.01000\n",
      "trial 800\t Mean Score: 39.855 \t with epsilon: 0.010\n",
      "trial 900\t Mean Score: 44.079 \t with epsilon: 0.010\n",
      "trial 999\t Mean Score: 17.435 \t with epsilon: 0.010Epsilon Decay Ratio: 0.900\n",
      "trial 0\t Mean Score: -90.170 \t with epsilon: 0.900\n",
      "trial 100\t Mean Score: -181.945 \t with epsilon: 0.010\n",
      "trial 200\t Mean Score: -47.002 \t with epsilon: 0.0100\n",
      "trial 300\t Mean Score: -19.501 \t with epsilon: 0.010\n",
      "trial 400\t Mean Score: -2.678 \t with epsilon: 0.0100\n",
      "trial 500\t Mean Score: 50.055 \t with epsilon: 0.010\n",
      "trial 600\t Mean Score: 60.791 \t with epsilon: 0.010\n",
      "trial 700\t Mean Score: 64.337 \t with epsilon: 0.010\n",
      "trial 800\t Mean Score: 30.902 \t with epsilon: 0.010\n",
      "trial 900\t Mean Score: 49.895 \t with epsilon: 0.010\n",
      "trial 999\t Mean Score: 58.589 \t with epsilon: 0.010Epsilon Decay Ratio: 0.990\n",
      "trial 0\t Mean Score: -95.510 \t with epsilon: 0.990\n",
      "trial 100\t Mean Score: -180.989 \t with epsilon: 0.362\n",
      "trial 200\t Mean Score: -125.329 \t with epsilon: 0.133\n",
      "trial 300\t Mean Score: 7.479 \t with epsilon: 0.049636\n",
      "trial 400\t Mean Score: 7.280 \t with epsilon: 0.018\n",
      "trial 500\t Mean Score: 7.968 \t with epsilon: 0.0104\n",
      "trial 600\t Mean Score: 34.679 \t with epsilon: 0.010\n",
      "trial 700\t Mean Score: 33.992 \t with epsilon: 0.010\n",
      "trial 800\t Mean Score: 61.998 \t with epsilon: 0.010\n",
      "trial 900\t Mean Score: 79.143 \t with epsilon: 0.010\n",
      "trial 999\t Mean Score: 100.027 \t with epsilon: 0.010Epsilon Decay Ratio: 0.995\n",
      "trial 0\t Mean Score: -113.516 \t with epsilon: 0.995\n",
      "trial 100\t Mean Score: -158.013 \t with epsilon: 0.603\n",
      "trial 200\t Mean Score: -74.075 \t with epsilon: 0.3658\n",
      "trial 300\t Mean Score: -29.294 \t with epsilon: 0.221\n",
      "trial 400\t Mean Score: 15.600 \t with epsilon: 0.1340\n",
      "trial 500\t Mean Score: 56.719 \t with epsilon: 0.081\n",
      "trial 600\t Mean Score: 64.517 \t with epsilon: 0.049\n",
      "trial 700\t Mean Score: 66.883 \t with epsilon: 0.030\n",
      "trial 800\t Mean Score: 86.836 \t with epsilon: 0.018\n",
      "trial 900\t Mean Score: 132.015 \t with epsilon: 0.011\n",
      "trial 999\t Mean Score: 183.949 \t with epsilon: 0.010Epsilon Decay Ratio: 1.000\n",
      "trial 0\t Mean Score: -126.631 \t with epsilon: 1.000\n",
      "trial 100\t Mean Score: -181.663 \t with epsilon: 1.000\n",
      "trial 200\t Mean Score: -174.888 \t with epsilon: 1.000\n",
      "trial 300\t Mean Score: -187.322 \t with epsilon: 1.000\n",
      "trial 400\t Mean Score: -194.144 \t with epsilon: 1.000\n",
      "trial 500\t Mean Score: -190.357 \t with epsilon: 1.000\n",
      "trial 600\t Mean Score: -179.468 \t with epsilon: 1.000\n",
      "trial 700\t Mean Score: -173.755 \t with epsilon: 1.000\n",
      "trial 800\t Mean Score: -161.144 \t with epsilon: 1.000\n",
      "trial 900\t Mean Score: -182.437 \t with epsilon: 1.000\n",
      "trial 999\t Mean Score: -194.606 \t with epsilon: 1.000"
     ]
    }
   ],
   "source": [
    "for epsilon_decay in  epsilon_decays:\n",
    "    print(\"Epsilon Decay Ratio: {:.3f}\\n\".format(epsilon_decay), end=\"\")\n",
    "    scores_epsilon_decay[epsilon_decay], epsilons_epsilon_decay[epsilon_decay] = train(epsilon_decay=epsilon_decay)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
